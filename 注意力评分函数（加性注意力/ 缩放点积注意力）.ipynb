{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPYus3meQVrNoU5sl03ADUW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Galaxy-Feng/repository-one/blob/main/%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AF%84%E5%88%86%E5%87%BD%E6%95%B0%EF%BC%88%E5%8A%A0%E6%80%A7%E6%B3%A8%E6%84%8F%E5%8A%9B/%20%E7%BC%A9%E6%94%BE%E7%82%B9%E7%A7%AF%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 新段落"
      ],
      "metadata": {
        "id": "lQaU2K6wXkoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "def masked_softmax(X, valid_lens):\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
        "                          value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                        device=X.device)[None, :] < valid_len[:, None]\n",
        "    X[~mask] = value\n",
        "    return X\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):\n",
        "        super(AdditiveAttention, self).__init__(**kwargs)\n",
        "        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)\n",
        "        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)\n",
        "        self.w_v = nn.Linear(num_hiddens, 1, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens):\n",
        "        queries, keys = self.W_q(queries), self.W_k(keys)\n",
        "        features = queries.unsqueeze(2) + keys.unsqueeze(1)\n",
        "        features = torch.tanh(features)\n",
        "        scores = self.w_v(features).squeeze(-1)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)  # 修改点\n",
        "\n",
        "\n",
        "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
        "# values的小批量，两个值矩阵是相同的\n",
        "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
        "    2, 1, 1)\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
        "                              dropout=0.1)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCCldFDwF6sT",
        "outputId": "ca384b55-404f-49ea-e2f2-8a89598b38d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sequence_mask(X, valid_len, value=0):\n",
        "    maxlen = X.size(1)\n",
        "    mask = torch.arange((maxlen), dtype=torch.float32,\n",
        "                        device=X.device)[None, :] < valid_len[:, None]\n",
        "    X[~mask] = value\n",
        "    return X\n",
        "\n",
        "def masked_softmax(X, valid_lens):\n",
        "    if valid_lens is None:\n",
        "        return nn.functional.softmax(X, dim=-1)\n",
        "    else:\n",
        "        shape = X.shape\n",
        "        if valid_lens.dim() == 1:\n",
        "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
        "        else:\n",
        "            valid_lens = valid_lens.reshape(-1)\n",
        "        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
        "                              value=-1e6)\n",
        "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout, **kwargs):\n",
        "        super(DotProductAttention, self).__init__(**kwargs)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens=None):\n",
        "        d = queries.shape[-1]\n",
        "        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)\n",
        "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
        "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
        "\n",
        "\n",
        "queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))\n",
        "# values的小批量，两个值矩阵是相同的\n",
        "values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(\n",
        "    2, 1, 1)\n",
        "valid_lens = torch.tensor([2, 6])\n",
        "\n",
        "attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,\n",
        "                              dropout=0.1)\n",
        "attention.eval()\n",
        "attention(queries, keys, values, valid_lens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNavaZxfGYtG",
        "outputId": "b282df43-4ca4-4ed1-c187-ae7301a31807"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],\n",
              "\n",
              "        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}